<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="OW-Rep: Open-World Object Detection with Instance Representation Learning for objects' Inter-Relationship">
  <meta name="keywords" content="OWOD, Representation, Instance">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OW-Rep: Open-World Object Detection with Instance Representation Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/brain_icon2.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://sunohlee.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <!-- <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div> -->
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> OW-Rep: Open-World Object Detection with Instance Representation Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sunohlee.github.io">Sunoh Lee</a>*<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://msjeon.me">Minsik Jeon</a>*<sup>1</sup>,
            </span>
            <span class="author-block">
              Jihong Min<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://junwon.me">Junwon Seo</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Agency for Defense Development,</span>
            <span class="author-block"><sup>2</sup>Carnegie Mellon University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2409.16073"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="http://arxiv.org/abs/2409.16073"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/Kzcsaiia2rA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Result Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- intor Figure-->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <img src="./static/images/OW-Rep_concept_img.jpg"
                  class="method-image is-shadow"
                  style="width: 75%; height: auto;"
                  alt="Interpolate start reference image."/>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Open World Object Detection (OWOD) addresses realistic scenarios where unseen object classes emerge, 
            enabling detectors trained on known classes to detect unknown objects and incrementally incorporate the knowledge they provide.
          </p>
          <p>
            While existing OWOD methods primarily focus on detecting unknown objects, 
            they often overlook the rich semantic relationships between detected objects, 
            which are essential for scene understanding and applications in open-world environments (e.g., open-world tracking and novel class discovery).
          </p>
          <p>
            In this paper, we extend the OWOD framework to jointly detect unknown objects and learn semantically rich instance embeddings, 
            enabling the detector to capture fine-grained semantic relationships between instances. To this end, we propose two modules 
            that leverage the rich and generalizable knowledge of Vision Foundation Models (VFM). 
            First, the Unknown Box Refine Module uses semantic masks from the Segment Anything Model to accurately localize unknown objects.
            The Embedding Transfer Module then distills instance-wise semantic similarities from VFM features to the detector's embeddings via a relaxed contrastive loss, 
            
            enabling the detector to learn a semantically meaningful and generalizable instance feature. 
          </p>
          <p>
            Extensive experiments show that our method significantly improves both unknown object detection and instance embedding quality, 
            while also enhancing performance in downstream tasks such as open-world tracking.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

    

<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        
        <!-- Method Figure-->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <img src="./static/images/Model_full.jpg"
                  class="method-image is-shadow"
                  style="width: 120%; max-width: none; margin-left: -10%; overflow-x: auto;"
                  alt="Interpolate start reference image."/>
          </div>
        </div>
        <!--/ Method Figure-->

        <!-- Interpolating. -->
        <div class="column" style="width: 120%; max-width: none; margin-left: -10%; margin-top: 20px;">
          <div class="content has-text-justified">
            <!-- <p>
              ▲ Overall Architecture of the Proposed Method.
            </p> -->
            <p>
              ▲ Our method extends OWOD by not only detecting unknown objects but also
              extracting semantically rich features. We adopt PROB as the baseline open-word object detector. 
              During training, the known and unknown proposals from PROB, with corresponding instance embeddings, 
              are fed into the proposed modules. The Unknown Box Refine Module improves the localization of unknown objects by treating 
              refined unknown boxes from SAM as pseudo ground truth. The Embedding Transfer Module extracts source embeddings by average pooling DINOv2 features within the refined unknown and known proposals. 
              Pairwise similarities between source embeddings are then computed and used as weights for the relaxed contrastive loss,
              controlling the attraction and repulsion between instance embeddings. At inference, the detector generates semantically rich instance
              embeddings, capturing fine-grained relationships between detected proposals.
            </p>
          </div>
        </div>
        <!-- <br/> -->
        <!--/ Interpolating. -->

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        
        <h3 class="title is-5">Qualitative results of inter-proposal relationships on OWOD and Unknown-Unknown split</h3>
        
        <div class="is-centered">

          <!-- Image Section (wider than text) -->
          <div class="column" style="position: relative; overflow: visible;">
            <img src="./static/images/sim_results.jpg"
                  class="method-image is-shadow"
                  style="width: 120%; max-width: none; margin-left: -10%; margin-top: -15px; margin-bottom: -15px;"
                  alt="Interpolate start reference image."/>
          </div>

          <!-- Text Section (fixed width) -->
          <!-- <div class="column is-four-fifths" style="margin: 0 auto;"> -->
          <div class="column" style="width: 120%; max-width: none; margin-left: -10%; margin-bottom: 30px;">
            <div class="content has-text-justified">
              <p>
                ▲ Proposals with high feature
                similarity to the reference are shown red, while dissimilar proposals are in blue. Ours successfully captures semantic similarities between
                both the known and unknown objects. For example, the reference giraffe is similar to both an unknown giraffe and a known horse, while
                the fire hydrant is highly dissimilar. In contrast, PROB treats all proposals as highly similar. RNCDL, despite using self-supervision to
                learn features, fails to capture meaningful semantics, mistakenly considering the giraffe and fire hydrant highly similar.
            </div>
          </div>
          
        </div>
        
        <h3 class="title is-5">t-SNE visualization of the learned instance embeddings</h3>
        <div class="is-centered">

          <!-- Image Section (wider than text) -->
          <div class="column" style="position: relative; overflow: visible;">
            <img src="./static/images/tsne_results.jpg"
                  class="method-image is-shadow"
                  style="width: 120%; max-width: none; margin-left: -10%; margin-top: -15px; margin-bottom: -15px;"
                  alt="Interpolate start reference image."/>
          </div>

          <!-- Text Section (fixed width) -->
          <!-- <div class="column is-four-fifths" style="margin: 0 auto;"> -->
            <div class="column" style="width: 120%; max-width: none; margin-left: -10%; margin-bottom: 30px;">
            <div class="content has-text-justified">
              <p>
                ▲ Unlike PROB, where embeddings are mixed and lack clear separation, 
                our method produces a well-structured feature space with distinct class-wise clusters. 
                Additionally, semantically similar animal classes appear closer together, 
                while unrelated categories remain separate. This shows that our method effectively
                captures semantic relationships within the feature space.
              </p>
            </div>
          </div>
          
        </div>

        <h3 class="title is-5">Qualitative results of unknown object detection</h3>
        <div class="is-centered">

          <!-- Image Section (wider than text) -->
          <div class="column" style="position: relative; overflow: visible;">
            <img src="./static/images/det_results.jpg"
                  class="method-image is-shadow"
                  style="width: 100%; max-width: none; margin-left: 0%; margin-top: -15px; margin-bottom: -15px;"
                  alt="Interpolate start reference image."/>
          </div>

          <!-- Text Section (fixed width) -->
          <!-- <div class="column is-four-fifths" style="margin: 0 auto;"> -->
          <div class="column" style="width: 100%; max-width: none; margin-left: 0%; margin-bottom: 30px;">
            <div class="content has-text-justified">
              <p>
                ▲ Unknown object detections from PROB (top row) and our model (bottom row) are compared. By leveraging semantic masks from
                SAM, our model achieves accurate localization.
              </p>
            </div>
          </div>
          
        </div>

        <h3 class="title is-5">Qualitative results in an open-world tracking scenario</h3>
        <div class="is-centered">

          <!-- Image Section (wider than text) -->
          <div class="column" style="position: relative; overflow: visible;">
            <img src="./static/images/tracking_results.jpg"
                  class="method-image is-shadow"
                  style="width: 100%; max-width: none; margin-left: 0%; margin-top: -15px; margin-bottom: -15px;"
                  alt="Interpolate start reference image."/>
          </div>

          <!-- Text Section (fixed width) -->
          <!-- <div class="column is-four-fifths" style="margin: 0 auto;"> -->
          <div class="column" style="width: 100%; max-width: none; margin-left: 0%; margin-bottom: 30px;">
            <div class="content has-text-justified">
              <p>
                ▲ Bounding box colors represent track IDs, with both the squirrel and ball belonging
                to unknown classes. While PROB successfully detects the squirrel, they
                fail to associate them across frames since the embeddings of squirrel’s
                varying shapes are not similar enough. Our method successfully tracks the
                squirrel by learning semantically rich instance embeddings, which are used
                to compute feature similarity between inter-frame proposals.
              </p>
            </div>
          </div>
          
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Result Video</h2>
        <div class="column" style="width: 100%; max-width: none; margin-left: 0%; margin-bottom: 30px;">
          <div class="content has-text-justified">
            <p>
              00:05-00:23 : Model explanation
            </p>
            <p>
              00:26-01:06 : Inter-Proposal Relationship Results (OWOD split)
            </p>
            <p>
              01:06-01:45 : Inter-Proposal Relationship Results (Unknown Unknown split)
            </p>
            <p>
              01:45-02:13 : Detection Results
            </p>
            <p>
              02:13-02:50 : Tracking Results
            </p>
          </div>
        </div>
        <div class="publication-video">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/Kzcsaiia2rA?si=kf1rWdr-8T320tJX" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered"> -->

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      <!-- </div>
    </div> -->
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->
<!-- 
  </div>
</section> -->


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
